{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "import csv\n",
    "import psycopg2 \n",
    "import json\n",
    "from typing import TypedDict\n",
    "load_dotenv()\n",
    "\n",
    "URL_DB = os.environ.get(\"URL_DB\")\n",
    "CONN = psycopg2.connect(URL_DB)\n",
    "CURSOR = CONN.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: GROUP LOCATION BY COUNTRIES. \n",
    "# NOTE: WHERE IS LOCATION == NONE?\n",
    "# NOTE: WHERE IS DESCRIPTION == NONE?\n",
    "\n",
    "\n",
    "## TODO: NEED TO TRANSFROM COMMON TWO WORDS SUCH AS UNITED STATES INTO \"USA\"; \"NEW YORK\", \n",
    "\n",
    "timestamp = \"2024-05-01 00:00:00.000000\"\n",
    "\n",
    "CURSOR.execute(\n",
    "\tf\"SELECT location FROM main_jobs  WHERE timestamp > '{timestamp}'\"\n",
    ")\n",
    "new_data = CURSOR.fetchall()\n",
    "df = pd.DataFrame(new_data)\n",
    "\n",
    "df.rename(columns={0: \"location\"}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_file(file_path: str):\n",
    "\twith open(file_path, 'r') as file:\n",
    "\t\treturn json.load(file)\n",
    "\n",
    "def save_json_file(data: dict, file_path: str) -> None:\n",
    "\twith open(file_path, 'w') as file:\n",
    "\t\tjson.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_file = \"/root/JobsCrawler/src/notebooks/all_locations.json\"\\noutput_file = \"/root/JobsCrawler/src/notebooks/all_locations_transformed.json\"\\n\\ninput_data = load_json_file(input_file)\\ntransformed_data = transform_data(input_data)\\nsave_json_file(transformed_data, output_file)\\n\\nprint(\"Data transformation complete. Result saved to\", output_file)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def transform_data(input_data: dict) -> dict[str, dict[str, list[dict[str, list[str]]]]]:\n",
    "\t\"\"\"Simplify the data structure of 'all_locations.json' for better and easier mapping\"\"\"\n",
    "\tresult = {}\n",
    "\tfor continent, data in input_data.items():\n",
    "\t\tresult[continent] = {\"Countries\": []}\n",
    "\t\tfor country in data[\"Countries\"]:\n",
    "\t\t\tcountry_name: str = country[\"country_name\"].upper()\n",
    "\t\t\tcountry_code = country[\"country_code\"].upper()\n",
    "\t\t\tif country[\"capital_english\"] != \"NaN\":\n",
    "\t\t\t\tcapital_english = country[\"capital_english\"].upper()\n",
    "\t\t\t\n",
    "\t\t\tsubdivisions = []\n",
    "\t\t\tif isinstance(country[\"subdivisions\"], list):\n",
    "\t\t\t\tsubdivisions = [sub[\"subdivisions_name\"].upper() for sub in country[\"subdivisions\"]]\n",
    "\t\t\telif country[\"subdivisions\"] != \"NaN\":\n",
    "\t\t\t\tsubdivisions = [country[\"subdivisions\"].upper()]\n",
    "\t\t\t\n",
    "\t\t\ttransformed_country = {\n",
    "\t\t\t\tcountry_name: [country_code, capital_english] + subdivisions\n",
    "\t\t\t}\n",
    "\t\t\tresult[continent][\"Countries\"].append(transformed_country)\n",
    "\t\n",
    "\treturn result\n",
    "\n",
    "#Example usage:\n",
    "\n",
    "\"\"\"\n",
    "input_file = \"/root/JobsCrawler/src/notebooks/all_locations.json\"\n",
    "output_file = \"/root/JobsCrawler/src/notebooks/all_locations_transformed.json\"\n",
    "\n",
    "input_data = load_json_file(input_file)\n",
    "transformed_data = transform_data(input_data)\n",
    "save_json_file(transformed_data, output_file)\n",
    "\n",
    "print(\"Data transformation complete. Result saved to\", output_file)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['original_index'] = df.index\n",
    "\n",
    "df[\"location\"] = df[\"location\"].str.replace(\",\", \"\", regex=False).str.replace(\")\", \"\", regex=False).str.replace(\"(\", \"\", regex=False).str.replace(\"|\", \" \", regex=False)\n",
    "\n",
    "df[\"location\"] = df[\"location\"].str.strip().str.split()\n",
    "df = df.explode(\"location\").reset_index(drop=True)\n",
    "\n",
    "df.head(25)\n",
    "\n",
    "df.to_csv(\"/root/JobsCrawler/src/notebooks/all_location_words.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique words\n",
    "unique_words = df[\"location\"].unique()\n",
    "\n",
    "# Create a new DataFrame with these unique words\n",
    "unique_words_df = pd.DataFrame({\"unique_word\": unique_words})\n",
    "\n",
    "\n",
    "unique_words_df.to_csv(\"/root/JobsCrawler/src/notebooks/unique_words.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## naive approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ndef add_location_tags(df: pd.DataFrame, json_file_path: str) -> pd.DataFrame:\\n\\tlocation_data = load_json_file(json_file_path)\\n\\tdef find_location_tag(word: str) -> str | float:\\n\\t\\t\\tword_str = str(word)\\n\\t\\t\\tfor continent, countries in location_data.items():\\n\\t\\t\\t\\tfor country in countries[\\'Countries\\']:\\n\\t\\t\\t\\t\\tfor country_name, locations in country.items():\\n\\t\\t\\t\\t\\t\\tfor loc in locations:\\n\\t\\t\\t\\t\\t\\t\\tif word_str.lower() in loc.lower():\\n\\t\\t\\t\\t\\t\\t\\t\\treturn country_name\\n\\t\\t\\t\\t\\t\\tif word_str.lower() == country_name.lower():\\n\\t\\t\\t\\t\\t\\t\\treturn country_name\\n\\t\\t\\t\\tif word_str.lower() == continent.lower():\\n\\t\\t\\t\\t\\treturn continent\\n\\t\\t\\treturn np.nan\\n\\tdf[\\'location_tag\\'] = df[0].apply(find_location_tag)\\n\\treturn df\\n\\njson_file_path = \\'/root/JobsCrawler/src/notebooks/all_locations_transformed.json\\'\\nresult2_df = add_location_tags(df, json_file_path)\\nprint(result2_df)\\n\\nresult2_df.to_csv(\"/root/JobsCrawler/src/notebooks/country_mapping.csv\")\\n\\nnan_count_per_column = result2_df.isna().sum()\\n\\nprint(\"nan_count_per_column\", nan_count_per_column)\\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "## TODO: AS SOME LOCATIONS CONTAIN MORE THAN A WORD, IT SHOULD ADD THE NEXT WORD IF THE RESULT IS UNKNOWN, AND IF THEIR INDEX ARE THE SAME. ELSE FINALLY ASSING UNKNOWN AGAIN. MAKE A FUNCTION THAT CONCATENATES THE WORD THAT COULD NOT BE FOUND IN LOC.LOWER AND ADD ITS NEXT WORD ONLY IF THEIR INDEX ARE EQUAL. FOR EXAMPLE, THIS FUNCTION WOULD NOT MATCH THE WORD \"SAN\" BUT IF WE DO THIS, THEN THE CONCATENATED STRING WOULD BE \"SAN FRANCISCO\", WHICH SHOULD FIND ITS MATCH.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def add_location_tags(df: pd.DataFrame, json_file_path: str) -> pd.DataFrame:\n",
    "\tlocation_data = load_json_file(json_file_path)\n",
    "\tdef find_location_tag(word: str) -> str | float:\n",
    "\t\t\tword_str = str(word)\n",
    "\t\t\tfor continent, countries in location_data.items():\n",
    "\t\t\t\tfor country in countries['Countries']:\n",
    "\t\t\t\t\tfor country_name, locations in country.items():\n",
    "\t\t\t\t\t\tfor loc in locations:\n",
    "\t\t\t\t\t\t\tif word_str.lower() in loc.lower():\n",
    "\t\t\t\t\t\t\t\treturn country_name\n",
    "\t\t\t\t\t\tif word_str.lower() == country_name.lower():\n",
    "\t\t\t\t\t\t\treturn country_name\n",
    "\t\t\t\tif word_str.lower() == continent.lower():\n",
    "\t\t\t\t\treturn continent\n",
    "\t\t\treturn np.nan\n",
    "\tdf['location_tag'] = df[0].apply(find_location_tag)\n",
    "\treturn df\n",
    "\n",
    "json_file_path = '/root/JobsCrawler/src/notebooks/all_locations_transformed.json'\n",
    "result2_df = add_location_tags(df, json_file_path)\n",
    "print(result2_df)\n",
    "\n",
    "result2_df.to_csv(\"/root/JobsCrawler/src/notebooks/country_mapping.csv\")\n",
    "\n",
    "nan_count_per_column = result2_df.isna().sum()\n",
    "\n",
    "print(\"nan_count_per_column\", nan_count_per_column)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sliding window approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      location  original_index location_tag\n",
      "0       Canada               0       CANADA\n",
      "1       Canada               1       CANADA\n",
      "2       Remote               2          NaN\n",
      "3       Hybrid               2          NaN\n",
      "4       Remote               2          NaN\n",
      "...        ...             ...          ...\n",
      "56840   Remote           11816          NaN\n",
      "56841    India           11817        INDIA\n",
      "56842  Chennai           11817          NaN\n",
      "56843    India           11817        INDIA\n",
      "56844   Remote           11817          NaN\n",
      "\n",
      "[56845 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Countries(TypedDict):\n",
    "\tcountry_name: str\n",
    "\tlocations: list[str]\n",
    "\n",
    "class WorldLocations(TypedDict):\n",
    "\tcontinent: str\n",
    "\tareas: list[str]\n",
    "\tcountries: list[Countries]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_location_tag(word: str, location_data: WorldLocations) -> str:\n",
    "\tword_upper = word.upper()\n",
    "\tfor continent, countries in location_data.items():\n",
    "\t\tif word_upper == continent.upper():\n",
    "\t\t\treturn continent\n",
    "\t\tfor zone in countries['Zones']:\n",
    "\t\t\tif word_upper == zone:\n",
    "\t\t\t\treturn continent\n",
    "\t\tfor country in countries['Countries']:\n",
    "\t\t\tfor country_name, locations in country.items():\n",
    "\t\t\t\tif word_upper == country_name or word_upper in [loc for loc in locations]:\n",
    "\t\t\t\t\treturn country_name\n",
    "\treturn \"\"\n",
    "\n",
    "def add_location_tags(df: pd.DataFrame, json_file_path: str) -> pd.DataFrame:\n",
    "\tlocation_data = load_json_file(json_file_path)\n",
    "\tresult = []\n",
    "\ti = 0\n",
    "\twhile i < len(df):\n",
    "\t\tcurrent_word = str(df.iloc[i, 0])\n",
    "\t\tcurrent_original_index = df.loc[i, \"original_index\"]\n",
    "\t\t\n",
    "\t\ttag = find_location_tag(current_word, location_data)\n",
    "\t\t\n",
    "\t\tif tag:\n",
    "\t\t\tresult.append(tag)\n",
    "\t\t\ti += 1\n",
    "\t\telse:\n",
    "\t\t\t# If no match, try to concatenate with the next word if it has the same original_index\n",
    "\t\t\tif i + 1 < len(df) and df.loc[i + 1, \"original_index\"] == current_original_index:\n",
    "\t\t\t\tnext_word = str(df.iloc[i + 1, 0])\n",
    "\t\t\t\tcompound_word = f\"{current_word} {next_word}\"\n",
    "\t\t\t\ttag = find_location_tag(compound_word, location_data)\n",
    "\t\t\t\t\n",
    "\t\t\t\tif tag:\n",
    "\t\t\t\t\tresult.extend([tag, tag])\n",
    "\t\t\t\t\ti += 2\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tresult.append(np.nan)\n",
    "\t\t\t\t\ti += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tresult.append(np.nan)\n",
    "\t\t\t\ti += 1\n",
    "\n",
    "\tdf['location_tag'] = result\n",
    "\treturn df\n",
    "\n",
    "json_file_path = '/root/JobsCrawler/src/notebooks/all_locations_transformed.json'\n",
    "result_df = add_location_tags(df, json_file_path)\n",
    "print(result_df)\n",
    "\n",
    "result_df.to_csv(\"/root/JobsCrawler/src/notebooks/country_mapping1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location            129\n",
      "original_index        0\n",
      "location_tag      17630\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nan_count_per_column = result_df.isna().sum()\n",
    "\n",
    "print(nan_count_per_column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do the same but for the unique words. To see all the words that are not currently mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_word</th>\n",
       "      <th>location_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canada</td>\n",
       "      <td>CANADA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Remote</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hybrid</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USA</td>\n",
       "      <td>UNITED STATES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Europe</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>Cencora</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>Tunis</td>\n",
       "      <td>TUNISIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>Tacoma</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>EUR3500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>6500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1048 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unique_word   location_tag\n",
       "0         Canada         CANADA\n",
       "1         Remote            NaN\n",
       "2         Hybrid            NaN\n",
       "3            USA  UNITED STATES\n",
       "4         Europe         Europe\n",
       "...          ...            ...\n",
       "1043     Cencora            NaN\n",
       "1044       Tunis        TUNISIA\n",
       "1045      Tacoma            NaN\n",
       "1046     EUR3500            NaN\n",
       "1047        6500            NaN\n",
       "\n",
       "[1048 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def simple_add_location_tags(df: pd.DataFrame, json_file_path: str) -> pd.DataFrame:\n",
    "\tlocation_data = load_json_file(json_file_path)\n",
    "\tresult = []\n",
    "\ti = 0\n",
    "\twhile i < len(df):\n",
    "\t\tcurrent_word = str(df.iloc[i, 0])\n",
    "\t\t\n",
    "\t\ttag = find_location_tag(current_word, location_data)\n",
    "\t\t\n",
    "\t\tif tag:\n",
    "\t\t\tresult.append(tag)\n",
    "\t\telse:\n",
    "\t\t\tresult.append(np.nan)\n",
    "\t\t\n",
    "\t\ti += 1\n",
    "\n",
    "\tdf['location_tag'] = result\n",
    "\treturn df\n",
    "\n",
    "json_file_path = '/root/JobsCrawler/src/notebooks/all_locations_transformed.json'\n",
    "\n",
    "\n",
    "unique_mapped_words_df = simple_add_location_tags(unique_words_df, json_file_path)\n",
    "\n",
    "unique_mapped_words_df.to_csv(\"/root/JobsCrawler/src/notebooks/unique_word_mapping.csv\")\n",
    "\n",
    "\n",
    "unique_mapped_words_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_word</th>\n",
       "      <th>location_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canada</td>\n",
       "      <td>CANADA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Remote</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hybrid</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USA</td>\n",
       "      <td>UNITED STATES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Europe</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>Cencora</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>Tunis</td>\n",
       "      <td>TUNISIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>Tacoma</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>EUR3500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>6500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1048 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unique_word   location_tag\n",
       "0         Canada         CANADA\n",
       "1         Remote            NaN\n",
       "2         Hybrid            NaN\n",
       "3            USA  UNITED STATES\n",
       "4         Europe         Europe\n",
       "...          ...            ...\n",
       "1043     Cencora            NaN\n",
       "1044       Tunis        TUNISIA\n",
       "1045      Tacoma            NaN\n",
       "1046     EUR3500            NaN\n",
       "1047        6500            NaN\n",
       "\n",
       "[1048 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_mapped_words_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_word       1\n",
      "location_tag    691\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nan_count_per_column = unique_mapped_words_df.isna().sum()\n",
    "\n",
    "print(nan_count_per_column)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
